\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{booktabs} % For professional tables
\usepackage{float}

% Title Configuration
\title{\textbf{Assignment 1: Paper Review \& Code Reproduction} \\ 
\large CSC415: Introduction to Reinforcement Learning}
\author{\textbf{Student Name:} Shijun Chen \\ 
\textbf{Student ID:} 1009305716}
\date{\today}

\begin{document}

\maketitle

\section*{Part I: Paper Reviews (50 Points)}

% =================================================================================
% REVIEW 1: DECISION TRANSFORMER
% =================================================================================
\subsection*{Review 1}
\textbf{Paper Title:} Decision Transformer: Reinforcement Learning via Sequence Modeling \\
\textbf{Authors:} Chen et al. (2021)

\subsubsection*{1. Summary of Contributions}
\begin{itemize}
    \item \textbf{The Problem:} Traditional Offline Reinforcement Learning relies on bootstrapping (TD-learning) to estimate value functions. This introduces the "deadly triad" of instability (off-policy data, bootstrapping, function approximation), leading to value overestimation and error propagation, particularly in sparse-reward or suboptimal data regimes.
    \item \textbf{The Core Method:} The authors propose the \textit{Decision Transformer} (DT), which reframes RL not as a dynamic programming problem, but as a conditional sequence modeling problem. They treat trajectories as sequences of tokens $\tau = (\hat{R}_1, s_1, a_1, \dots, \hat{R}_T, s_T, a_T)$. The model uses a GPT architecture to autoregressively predict the next action $a_t$ given the history of states and a \textit{Target Return} (Return-to-Go, $\hat{R}_t$). This removes the need for value functions entirely.
    \item \textbf{The Result:} DT matches or exceeds the performance of state-of-the-art model-free offline RL algorithms (like CQL and BEAR) on standard benchmarks (Atari, OpenAI Gym). Crucially, it demonstrates "stitching" abilities—combining suboptimal trajectory segments to produce expert behavior—without explicit Bellman updates.
\end{itemize}

\subsubsection*{2. Strengths}
\begin{itemize}
    \item \textbf{Theoretical (Stability via Supervised Learning):} By casting RL as a supervised learning problem, DT avoids the optimization instability inherent in Bellman backups (e.g., the max-operator bias in Q-learning). It leverages the stable, well-understood convergence properties of the Cross-Entropy loss used in language modeling.
    \item \textbf{Empirical (Long-Term Credit Assignment):} The Transformer architecture utilizes self-attention ($O(N^2)$), allowing it to directly credit actions to distant rewards. This is demonstrated in the \textit{Key-to-Door} environment, where DT solves tasks requiring credit assignment over long horizons where TD-learning methods fail due to vanishing gradients in the value chain.
\end{itemize}

\subsubsection*{3. Weaknesses}
\begin{itemize}
    \item \textbf{Stochasticity \& Multimodality:} DT is inherently deterministic in its standard formulation (predicting the mean action or a single token). In highly stochastic environments where the optimal policy is multimodal (e.g., "go left or right, but not straight"), minimizing Mean Squared Error (MSE) leads to averaging valid modes, resulting in catastrophic failure (e.g., going straight into an obstacle).
    \item \textbf{Dependence on Training Distribution (No Ex-Nihilo Exploration):} Unlike Q-learning, which can theoretically propagate values to unvisited states via generalization, DT is strictly bounded by the support of the training data. It cannot hallucinate a "better" return than the maximum return present in the dataset unless it stitches existing segments; it cannot discover truly novel strategies.
    \item \textbf{Computational Complexity:} Inference is autoregressive. Predicting a sequence of length $T$ requires $O(T)$ forward passes, which is significantly slower ($\sim$10-100ms) than a standard MLP policy ($\sim$1ms), making it difficult to deploy in high-frequency robotic control loops (e.g., 500Hz).
\end{itemize}

\subsubsection*{4. Proposed Improvements}
\begin{itemize}
    \item \textbf{Proposal 1 (Addressing Multimodality):} Replace the deterministic MSE loss with a \textit{Diffusion Head}. Instead of predicting a point estimate $\hat{a}_t$, the Transformer would condition a diffusion model $p_\theta(a_t | s_t, \hat{R}_t)$ to denoise random noise into an action. This allows the model to represent complex, multimodal policy distributions, preventing mode-collapse in stochastic environments.
    \item \textbf{Proposal 2 (Addressing Generalization):} Introduce \textit{Hindsight Experience Replay (HER)} for returns. During training, relabel the target return $\hat{R}_t$ of a trajectory with values slightly \textit{higher} than actually achieved, while weighting these samples by their reachability. This could encourage the model to learn a generalized conditional distribution that extrapolates slightly beyond the dataset's maximum return.
\end{itemize}

\newpage

% =================================================================================
% REVIEW 2: DIFFUSER (THE RIVAL PAPER)
% =================================================================================
\subsection*{Review 2}
\textbf{Paper Title:} Planning with Diffusion for Flexible Behavior Synthesis \\
\textbf{Authors:} Janner et al. (2022)

\subsubsection*{1. Summary of Contributions}
\begin{itemize}
    \item \textbf{The Problem:} Autoregressive models (like Decision Transformer) generate actions step-by-step, which is myopic and prone to compounding errors ("exposure bias"). Furthermore, they cannot easily handle "inpainting" constraints (e.g., "reach goal X while passing through point Y").
    \item \textbf{The Core Method:} The authors introduce \textit{Diffuser}, which treats trajectory generation as a denoising diffusion probabilistic model (DDPM). Instead of predicting the next action, Diffuser generates the \textit{entire} trajectory ($\tau$) simultaneously by iteratively refining Gaussian noise. Planning becomes a sampling process where the gradients of a reward function guide the denoising steps.
    \item \textbf{The Result:} Diffuser outperforms autoregressive baselines on long-horizon tasks and enables "flexible behavior synthesis"—the ability to modify plans in real-time by applying constraints as gradients during the sampling process (e.g., avoiding a sudden obstacle).
\end{itemize}

\subsubsection*{2. Strengths}
\begin{itemize}
    \item \textbf{Theoretical (Global Consistency):} By generating the entire trajectory at once, Diffuser ensures global consistency. Unlike DT, which might make a greedy choice at $t=0$ that leads to a dead-end at $t=100$, Diffuser's joint distribution modeling ensures that the state at $t=0$ is compatible with the goal state at $t=100$.
    \item \textbf{Practical (Compositionality):} Different constraints (e.g., "smoothness" + "avoid obstacle" + "maximize reward") can be composed simply by summing their gradients during the denoising step. This allows for zero-shot generalization to new tasks without retraining the model.
\end{itemize}

\subsubsection*{3. Weaknesses}
\begin{itemize}
    \item \textbf{Inference Speed:} Diffusion models are notoriously slow. Diffuser typically requires 20-100 denoising steps to generate a single plan. This introduces seconds of latency, which is unacceptable for real-time dynamic agents (e.g., a robot catching a ball).
    \item \textbf{Horizon Scaling:} The dimensionality of the generation problem scales linearly with the planning horizon ($H \times \text{dim}(S+A)$). For extremely long horizons (e.g., 10,000 steps), the joint distribution becomes too high-dimensional to learn effectively without hierarchical abstraction.
\end{itemize}

\subsubsection*{4. Proposed Improvements}
\begin{itemize}
    \item \textbf{Proposal 1 (Addressing Speed):} Implement \textit{Consistency Distillation}. Train a "student" model to predict the result of multiple diffusion steps in a single forward pass. This would reduce the inference cost from $N$ steps to 1-2 steps, bridging the gap between Diffuser's flexibility and DT's speed.
    \item \textbf{Proposal 2 (Addressing Horizon):} Integrate a \textit{Latent World Model}. Instead of diffusing in raw high-dimensional observation space (pixels), perform diffusion in a compressed latent space learned by a VQ-VAE. This reduces the dimensionality of the planning problem, allowing for longer effective planning horizons.
\end{itemize}

\newpage

% =================================================================================
% PART II: CODE REPRODUCTION
% =================================================================================
\section*{Part II: Code Reproduction (45 Points)}

\textbf{Selected Paper:} Decision Transformer: Reinforcement Learning via Sequence Modeling \\
\textbf{GitHub Repository:} \url{https://github.com/CSJ111/415_DT_Reproduction}

\subsection*{1. Implementation Details}
I reproduced the Decision Transformer on the \texttt{Hopper-Medium-v2} environment from the D4RL benchmark. The implementation was optimized for high-performance compute (NVIDIA A100) to allow for rapid iteration.

\begin{itemize}
    \item \textbf{Architecture:} A GPT-2 style Transformer with causal masking.
    \begin{itemize}
        \item \textbf{Embedding Dimension:} 512 (Scaled up from the paper's 128 for better capacity).
        \item \textbf{Layers/Heads:} 4 Layers, 4 Attention Heads.
        \item \textbf{Context Length ($K$):} 20 timesteps.
    \end{itemize}
    \item \textbf{Optimization:}
    \begin{itemize}
        \item \textbf{Batch Size:} 4096 (Optimized for A100 saturation).
        \item \textbf{Optimizer:} AdamW with a learning rate of $6e-4$ and linear warmup.
        \item \textbf{Speedup Techniques:} Utilized \texttt{torch.compile} (JIT) and \texttt{torch.amp.autocast} (Mixed Precision) to achieve training times of $<10$ minutes for 2,500 gradient steps (equivalent to 150k steps at standard batch sizes).
    \end{itemize}
    \item \textbf{Dataset:} \texttt{hopper-medium-v2}. I implemented a custom "GPU-Resident" dataset class that loads the entire HDF5 file into VRAM at initialization to eliminate CPU-GPU data transfer bottlenecks.
\end{itemize}

\subsection*{2. Ablation Study Description}
I performed two ablation studies to verify the "Sequence Modeling" hypothesis:
\begin{enumerate}
    \item \textbf{Return Conditioning (Prompt Sensitivity):} I hypothesized that if the model is truly conditional, varying the \textit{Target Return} prompt at test time should result in predictable changes in agent performance. I evaluated the same trained model with targets of $\{3600, 1800, 400\}$.
    \item \textbf{Temporal Context ($K$):} I hypothesized that the model relies on history to "stitch" trajectories. I trained a separate model with context length $K=1$ (effectively a Markovian MLP) and compared it to the baseline $K=20$.
\end{enumerate}

\subsection*{3. Results \& Analysis}

\subsubsection*{Main Reproduction Results}
My reproduction successfully achieved expert-level performance on the medium dataset.

\begin{table}[h]
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Metric} & \textbf{Paper Baseline (128-dim)} & \textbf{My Reproduction (512-dim)} \\
\midrule
Mean Norm. Score & $67.6 \pm 1.0$ & $\mathbf{80.63 \pm 25.94}$ \\
Max Score & $\sim 75$ & $\mathbf{96.08}$ \\
Training Time & $\sim 30$ mins & $\mathbf{\sim 10}$ \textbf{mins} \\
\bottomrule
\end{tabular}
\caption{Comparison of reproduction results against the original paper baseline for Hopper-Medium.}
\label{tab:main_results}
\end{table}

\textbf{Analysis:} My mean score (80.63) exceeds the paper's baseline (67.6). I attribute this to the increased embedding dimension (512 vs 128), which allows the model to better resolve the "multimodal" nature of the medium dataset (separating the few good segments from the many bad ones). The high standard deviation ($\pm 25.9$) confirms the bimodal nature of the Hopper task: the agent either successfully stitches a trajectory (Score $>90$) or fails due to a physics instability (Score $<30$).

\subsubsection*{Ablation Analysis}

\begin{table}[h]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Ablation Type} & \textbf{Setting} & \textbf{Mean Score} & \textbf{Std Dev} \\
\midrule
\textbf{Conditioning} & Target = 3600 (Expert) & \textbf{80.63} & 25.94 \\
 & Target = 1800 (Medium) & 55.38 & 12.97 \\
 & Target = 400 (Low) & 26.71 & 0.26 \\
\midrule
\textbf{Context Length} & $K=20$ (Sequence) & \textbf{80.63} & 25.94 \\
 & $K=1$ (Markovian) & 31.20 & -- \\
\bottomrule
\end{tabular}
\caption{Ablation study results demonstrating the impact of Return Conditioning and Context Length.}
\label{tab:ablation}
\end{table}

\textbf{Interpretation:}
\begin{itemize}
    \item \textbf{Return Conditioning:} The results show a near-linear correlation between the requested target and the observed performance. Notably, the standard deviation collapses as the target decreases ($\pm 0.26$ for Target=400). This suggests that "failing" is easy to model consistently, whereas expert stitching requires navigating a narrow, unstable manifold.
    \item \textbf{Context Length:} The drop in performance from 80.63 ($K=20$) to 31.20 ($K=1$) is catastrophic. The $K=1$ model performs equivalent to the dataset average (Behavioral Cloning). This proves that the Decision Transformer requires temporal history to identify which "sub-policy" (expert vs. failure) it is currently executing, validating the sequence modeling hypothesis.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{output.png} 
    \caption{Training loss curve showing convergence in $<2500$ steps. The score is evaluated every 100 steps, demonstrating rapid improvement in the early phase of training.}
    \label{fig:results}
\end{figure}

\subsection*{AI Usage Declaration}
For README and report writing, I used Gemini 3 pro to assist in structuring the report, interpreting the results, and ensuring clarity in the presentation of the ablation study.
For implementation debugging, I used Gemini 3 pro to assist in debugging PyTorch tensor shape mismatches during the implementation of the GPU-resident dataset class.
Also used Gemini 3 pro to help interpret the results of the ablation study, particularly in understanding the relationship between target return and performance variance.

\end{document}